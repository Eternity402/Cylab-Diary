{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet_example_code_summary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnRGVrRtThwfnUghbs15X6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eternity402/Cylab-Diary/blob/master/resnet_example_code_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSK3-nWv2CBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this code is from https://github.com/weiaicunzai/pytorch-cifar100.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
        "    \"\"\"\n",
        "\n",
        "    #BasicBlock and BottleNeck block \n",
        "    #have different output size\n",
        "    #we use class attribute expansion\n",
        "    #to distinct\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        #residual function\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "        )\n",
        "\n",
        "        #shortcut\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        #the shortcut output dimension is not the same with residual function\n",
        "        #use 1*1 convolution to match the dimension\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"Residual block for resnet over 50 layers\n",
        "    \"\"\"\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "    \n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_block, num_classes=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True))\n",
        "        #we use a different inputsize than the original paper\n",
        "        #so conv2_x's stride is 1\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the \n",
        "        same as a neuron netowork layer, ex. conv layer), one layer may \n",
        "        contain more than one residual block \n",
        "        Args:\n",
        "            block: block type, basic block or bottle neck block\n",
        "            out_channels: output depth channel number of this layer\n",
        "            num_blocks: how many blocks per layer\n",
        "            stride: the stride of the first block of this layer\n",
        "        \n",
        "        Return:\n",
        "            return a resnet layer\n",
        "        \"\"\"\n",
        "\n",
        "        # we have num_block blocks per layer, the first block \n",
        "        # could be 1 or 2, other blocks would always be 1\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "        \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        output = self.conv3_x(output)\n",
        "        output = self.conv4_x(output)\n",
        "        output = self.conv5_x(output)\n",
        "        output = self.avg_pool(output)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output \n",
        "\n",
        "def resnet18():\n",
        "    \"\"\" return a ResNet 18 object\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "def resnet34():\n",
        "    \"\"\" return a ResNet 34 object\n",
        "    \"\"\"\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "def resnet50():\n",
        "    \"\"\" return a ResNet 50 object\n",
        "    \"\"\"\n",
        "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
        "\n",
        "def resnet101():\n",
        "    \"\"\" return a ResNet 101 object\n",
        "    \"\"\"\n",
        "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
        "\n",
        "def resnet152():\n",
        "    \"\"\" return a ResNet 152 object\n",
        "    \"\"\"\n",
        "    return ResNet(BottleNeck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y7EcNRjqo0b",
        "colab_type": "code",
        "outputId": "4003c252-a275-42a6-abbc-0ecc780cd62d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torchsummary\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "model = resnet18()\n",
        "# get cifar100 dataset\n",
        "'''transformer = transforms.Compose([transforms.Resize((32, 32)),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                 ])\n",
        "a = torchvision.datasets.CIFAR100('/root', train = True,download=True, transform=transformer)\n",
        "'''\n",
        "summary(model,(3,32,32))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "            Conv2d-7           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-8           [-1, 64, 32, 32]             128\n",
            "        BasicBlock-9           [-1, 64, 32, 32]               0\n",
            "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
            "             ReLU-12           [-1, 64, 32, 32]               0\n",
            "           Conv2d-13           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-14           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-15           [-1, 64, 32, 32]               0\n",
            "           Conv2d-16          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-17          [-1, 128, 16, 16]             256\n",
            "             ReLU-18          [-1, 128, 16, 16]               0\n",
            "           Conv2d-19          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-20          [-1, 128, 16, 16]             256\n",
            "           Conv2d-21          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-22          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-23          [-1, 128, 16, 16]               0\n",
            "           Conv2d-24          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-25          [-1, 128, 16, 16]             256\n",
            "             ReLU-26          [-1, 128, 16, 16]               0\n",
            "           Conv2d-27          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-28          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-29          [-1, 128, 16, 16]               0\n",
            "           Conv2d-30            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-31            [-1, 256, 8, 8]             512\n",
            "             ReLU-32            [-1, 256, 8, 8]               0\n",
            "           Conv2d-33            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
            "           Conv2d-35            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-37            [-1, 256, 8, 8]               0\n",
            "           Conv2d-38            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-39            [-1, 256, 8, 8]             512\n",
            "             ReLU-40            [-1, 256, 8, 8]               0\n",
            "           Conv2d-41            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-42            [-1, 256, 8, 8]             512\n",
            "       BasicBlock-43            [-1, 256, 8, 8]               0\n",
            "           Conv2d-44            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-46            [-1, 512, 4, 4]               0\n",
            "           Conv2d-47            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-48            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-49            [-1, 512, 4, 4]         131,072\n",
            "      BatchNorm2d-50            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-51            [-1, 512, 4, 4]               0\n",
            "           Conv2d-52            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-53            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-54            [-1, 512, 4, 4]               0\n",
            "           Conv2d-55            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
            "       BasicBlock-57            [-1, 512, 4, 4]               0\n",
            "AdaptiveAvgPool2d-58            [-1, 512, 1, 1]               0\n",
            "           Linear-59                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 11,220,132\n",
            "Trainable params: 11,220,132\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 13.63\n",
            "Params size (MB): 42.80\n",
            "Estimated Total Size (MB): 56.44\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
